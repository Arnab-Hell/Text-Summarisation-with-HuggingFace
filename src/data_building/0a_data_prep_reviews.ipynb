{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "009fe68e",
   "metadata": {},
   "source": [
    "# Part 0 - Data preparation\n",
    "\n",
    "In this notebook we will download the Amazon Review dataset and save it to S3. We will also do some light data preprocessing by only keeping the columns we need, filtering out reviews that are too short, and limiting the size of the datasets.\n",
    "\n",
    "To read more, please check out https://towardsdatascience.com/setting-up-a-text-summarisation-project-introduction-526622eea4a8."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23df895e",
   "metadata": {},
   "source": [
    "## Data download\n",
    "\n",
    "We download the dataset from https://huggingface.co/datasets/amazon_reviews_multi and save it to a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "687ff753",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f824c3aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\datasets\\load.py:1461: FutureWarning: The repository for amazon_reviews_multi contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/amazon_reviews_multi\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5554204002a64b0faeca21687923590a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4b444ae5f984ec4b45031f66e4a3ab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/15.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "DefunctDatasetError",
     "evalue": "Dataset 'amazon_reviews_multi' is defunct and no longer accessible due to the decision of data providers",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDefunctDatasetError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m----> 2\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamazon_reviews_multi\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m val_ds \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamazon_reviews_multi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m test_ds \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamazon_reviews_multi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\load.py:2556\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[0;32m   2551\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[0;32m   2552\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[0;32m   2553\u001b[0m )\n\u001b[0;32m   2555\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[1;32m-> 2556\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m load_dataset_builder(\n\u001b[0;32m   2557\u001b[0m     path\u001b[38;5;241m=\u001b[39mpath,\n\u001b[0;32m   2558\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[0;32m   2559\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39mdata_dir,\n\u001b[0;32m   2560\u001b[0m     data_files\u001b[38;5;241m=\u001b[39mdata_files,\n\u001b[0;32m   2561\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   2562\u001b[0m     features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[0;32m   2563\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n\u001b[0;32m   2564\u001b[0m     download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n\u001b[0;32m   2565\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m   2566\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   2567\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   2568\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[0;32m   2569\u001b[0m     _require_default_config_name\u001b[38;5;241m=\u001b[39mname \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2570\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_kwargs,\n\u001b[0;32m   2571\u001b[0m )\n\u001b[0;32m   2573\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[0;32m   2574\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\load.py:2265\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[1;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[0;32m   2263\u001b[0m builder_cls \u001b[38;5;241m=\u001b[39m get_dataset_builder_class(dataset_module, dataset_name\u001b[38;5;241m=\u001b[39mdataset_name)\n\u001b[0;32m   2264\u001b[0m \u001b[38;5;66;03m# Instantiate the dataset builder\u001b[39;00m\n\u001b[1;32m-> 2265\u001b[0m builder_instance: DatasetBuilder \u001b[38;5;241m=\u001b[39m builder_cls(\n\u001b[0;32m   2266\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   2267\u001b[0m     dataset_name\u001b[38;5;241m=\u001b[39mdataset_name,\n\u001b[0;32m   2268\u001b[0m     config_name\u001b[38;5;241m=\u001b[39mconfig_name,\n\u001b[0;32m   2269\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39mdata_dir,\n\u001b[0;32m   2270\u001b[0m     data_files\u001b[38;5;241m=\u001b[39mdata_files,\n\u001b[0;32m   2271\u001b[0m     \u001b[38;5;28mhash\u001b[39m\u001b[38;5;241m=\u001b[39mdataset_module\u001b[38;5;241m.\u001b[39mhash,\n\u001b[0;32m   2272\u001b[0m     info\u001b[38;5;241m=\u001b[39minfo,\n\u001b[0;32m   2273\u001b[0m     features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[0;32m   2274\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   2275\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   2276\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbuilder_kwargs,\n\u001b[0;32m   2277\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_kwargs,\n\u001b[0;32m   2278\u001b[0m )\n\u001b[0;32m   2279\u001b[0m builder_instance\u001b[38;5;241m.\u001b[39m_use_legacy_cache_dir_if_possible(dataset_module)\n\u001b[0;32m   2281\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\builder.py:382\u001b[0m, in \u001b[0;36mDatasetBuilder.__init__\u001b[1;34m(self, cache_dir, dataset_name, config_name, hash, base_path, info, features, token, use_auth_token, repo_id, data_files, data_dir, storage_options, writer_batch_size, name, **config_kwargs)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;66;03m# TODO FOR PACKAGED MODULES IT IMPORTS DATA FROM src/packaged_modules which doesn't make sense\u001b[39;00m\n\u001b[0;32m    381\u001b[0m     info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_exported_dataset_info()\n\u001b[1;32m--> 382\u001b[0m     info\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    383\u001b[0m info\u001b[38;5;241m.\u001b[39mbuilder_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m    384\u001b[0m info\u001b[38;5;241m.\u001b[39mdataset_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_name\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\amazon_reviews_multi\\30d298e0990e8a7e143dc917665441883d7e0b6a64516ef7eea2e89c1af1755c\\amazon_reviews_multi.py:91\u001b[0m, in \u001b[0;36mAmazonReviewsMulti._info\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_info\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 91\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DefunctDatasetError(\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamazon_reviews_multi\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is defunct and no longer accessible due to the decision of data providers\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m     )\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m datasets\u001b[38;5;241m.\u001b[39mDatasetInfo(\n\u001b[0;32m     95\u001b[0m         description\u001b[38;5;241m=\u001b[39m_DESCRIPTION,\n\u001b[0;32m     96\u001b[0m         features\u001b[38;5;241m=\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mFeatures(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    111\u001b[0m         citation\u001b[38;5;241m=\u001b[39m_CITATION,\n\u001b[0;32m    112\u001b[0m     )\n",
      "\u001b[1;31mDefunctDatasetError\u001b[0m: Dataset 'amazon_reviews_multi' is defunct and no longer accessible due to the decision of data providers"
     ]
    }
   ],
   "source": [
    "#from datasets import load_dataset\n",
    "#train_ds = load_dataset(\"amazon_reviews_multi\", \"en\", split='train')\n",
    "#val_ds = load_dataset(\"amazon_reviews_multi\", \"en\", split='validation')\n",
    "#test_ds = load_dataset(\"amazon_reviews_multi\", \"en\", split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa10c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_train = pd.read_csv\n",
    "df_val = pd.read_csv(val_ds)\n",
    "df_test = pd.read_csv(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ba9364",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f429804d",
   "metadata": {},
   "source": [
    "## Filtering the dataset\n",
    "\n",
    "We want to discard reviews and titles that are too short, so that our model can produce more interesting summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3534fb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_title = 5\n",
    "cutoff_body = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32d7087",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[(df_train['review_title'].apply(lambda x: len(x.split()) >= cutoff_title)) & (df_train['review_body'].apply(lambda x: len(x.split()) >= cutoff_body))]\n",
    "df_val = df_val[(df_val['review_title'].apply(lambda x: len(x.split()) >= cutoff_title)) & (df_val['review_body'].apply(lambda x: len(x.split()) >= cutoff_body))]\n",
    "df_test = df_test[(df_test['review_title'].apply(lambda x: len(x.split()) >= cutoff_title)) & (df_test['review_body'].apply(lambda x: len(x.split()) >= cutoff_body))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541dff26",
   "metadata": {},
   "source": [
    "## Limiting the size of the datasets\n",
    "\n",
    "We want to limit the size of the datasets so that training of the model can finish in a reasonable amount of time. This is a decision that we might want to revisit in the experimentation phase if we want to increase the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5228ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_train), len(df_val), len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c058601a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.sample(20000, random_state=42)\n",
    "df_train = df_train.rename(columns={\"review_body\": \"text\", \"review_title\": \"summary\"})\n",
    "\n",
    "df_val = df_val.sample(1000, random_state=42)\n",
    "df_val = df_val.rename(columns={\"review_body\": \"text\", \"review_title\": \"summary\"})\n",
    "\n",
    "df_test = df_test.sample(1000, random_state=42)\n",
    "df_test = df_test.rename(columns={\"review_body\": \"text\", \"review_title\": \"summary\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6326cf7",
   "metadata": {},
   "source": [
    "## Save the data as CSV files and upload them to S3\n",
    "\n",
    "We need to upload the data to S3 in order to train the model at a later point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4537321c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('data/train.csv', index=False, columns=['text', 'summary'])\n",
    "df_val.to_csv('data/val.csv', index=False, columns=['text', 'summary'])\n",
    "df_test.to_csv('data/test.csv', index=False, columns=['text', 'summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbb44f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c44d9f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install naas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6c098af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586092bb",
   "metadata": {},
   "source": [
    "### before creating boto3 client, you need to configure aws from the aws cli.\n",
    "### To do this, \n",
    "- Download and install aws cli file, check version by executing the command \"aws --version\" from cmd.\n",
    "- Sign in to AWS console. Open IAM. Create a new user. Click on attach policies directly and give \"administrator access\"\n",
    "- After user is created, go to \"Security credentials\" and generate access key. use case: CLI. Download the key csv file.\n",
    "- Go to command prompt and type \"aws configure\". Give all access key information, region name, and output format=\"json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c71b7ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    aws_access_key_id='YOUR_ID',\n",
    "    aws_secret_access_key='YOUR_KEY',\n",
    "    region_name='YOUR_REGION'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c631d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ap-south-1\n"
     ]
    }
   ],
   "source": [
    "my_region = boto3.session.Session().region_name # set the region of the instance\n",
    "print(my_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ab2a01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\ProgramData\\sagemaker\\sagemaker\\config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\Users\\ASUS\\AppData\\Local\\sagemaker\\sagemaker\\config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a0e1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Next step is to upload the datasets (train, val, test) to a storage bucket in S3.\n",
    "## You can do this from the console by creating a new bucket and uploading the relevant files, OR\n",
    "## You can use the AWS CLI to upload the files. \n",
    "## The CLI commands are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92aab613",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'aws' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp C:/Users/ASUS/Desktop/data/train.csv s3://\"bucket_name\"/summarization/data/train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18e4a3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uploading a file to S3, in other words copying a file from your local file system to S3, is done with aws s3 cp command\n",
    "#Let's suppose that your file name is file.txt  and this is how you can upload your file to S3\n",
    "#aws s3 cp file.txt s3://bucket-name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24643f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp C:/Users/ASUS/Desktop/data/val.csv s3://\"bucket_name\"/summarization/data/val.csv\n",
    "!aws s3 cp C:/Users/ASUS/Desktop/data/test.csv s3://\"bucket_name\"/summarization/data/test.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
